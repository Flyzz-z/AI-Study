# 机器学习

## 线性模型

## 决策树与CART

信息熵：对于一个包含K个类别的数据集D，其信息熵 Ent(D) 为：$Ent(D) = -Σ (pk * log₂(pk))$（k从1到K）。

信息增益：$Gain(D, a) = Ent(D) - Σ (|Dᵥ| / |D|) * Ent(Dᵥ)$,`v是特征a的所有可能取值。 即**划分前的熵 - 划分后各子集的熵的加权平均**

### ID3

多叉树，选择特征将数据集划分为多少子集就有几叉。

<img src="./assets/ID3.png" alt="ID3" style="zoom: 25%;" />

### CART

同时解决分类和回归问题，二叉树。

<img src="./assets/CART.png" alt="CART" style="zoom:25%;" />

- **分类任务：基尼指数**

  基尼指数衡量的是从数据集中随机抽取两个样本，其类别标签不一致的概率。基尼指数越小，数据集的纯度越高。

  其计算公式为：$Gini(p) = 1 - Σ(p_k²)$，其中 `p_k`是样本属于第k类的概率。

  在分裂时，算法会计算每个可能的分裂点所产生的子集的**加权基尼指数**，并选择使该值最小的特征和分裂点。

- **回归任务：平方误差**

  对于回归问题，CART算法采用最小二乘准则，即最小化每个节点内样本的**平方误差和**。

  节点分裂的目标是找到那个特征和分裂点，使得分裂后左右两个子节点的平方误差之和最小。平方误差的计算公式通常为：$MSE = (1/N) * Σ(y_i - ȳ)^2$，其中 `ȳ`是该节点中所有样本目标值的均值。

**关于预剪枝**

预剪枝是一种“未雨绸缪”的防治策略，它在决策树的**构建过程中**就进行干预。通过预先设定一些规则（如树的最大深度、节点最小样本数等），在决定一个节点是否分裂前进行评估。如果分裂带来的性能提升不显著或不符合规则，则立即停止分裂，将其标记为叶节点。这种方法的优势在于**效率高、实现简单**，能有效防止过拟合。

**关于后剪枝**

后剪枝则是一种“先发展后治理”的优化策略，它**先放任决策树完全生长**（甚至允许其过拟合），然后再**自底向上**地对已生成的树进行修剪。它会评估各个子树，若将其替换为叶节点能提升模型在验证集上的泛化性能，则进行剪枝。这种方法的优势在于它具备了“全局视野”，保留了大量潜在分支的可能，因此得到的模型通常**泛化能力更强、性能更优**。

<img src="./assets/剪枝.png" alt="剪枝" style="zoom: 33%;" />

## 随机森林与GBDT

### Bagging

随机森林选择的是一种策略。它**不依赖于一棵“完美”的决策树**，而是**训练一大批（比如100棵或500棵）各具特色的决策树**，组成一个“森林”。当需要预测新数据时，让森林里的每棵树都进行投票（分类问题）或取平均值（回归问题），最终的预测结果由“多数票”或“平均意见”决定。这种方式能有效降低单一模型可能产生的过拟合风险，提升整体模型的稳定性和泛化能力。

<img src="./assets/随机森林.png" alt="随机森林" style="zoom: 33%;" />

两种随机性保证树之间的差异性：

1. 数据选择随机：对于每棵树的训练，它不是使用全部的数据集，而是**有放回地随机抽取一部分样本**。这种方法称为**Bootstrap抽样**。

2. **特征的随机选择**：在每棵树的每个节点进行分裂时，它不是考虑所有特征来寻找最佳分裂点，而是**随机的从全部特征中选取一部分特征候选**，每个树学习到的模式不同。

### Boosting

GBDT（Gradient Boosting Decision Tree，梯度提升决策树）是一种非常强大且应用广泛的机器学习算法，它通过**组合多个简单的决策树（弱学习器）**，来构建一个强大的预测模型。其核心思想是**每一棵新树都致力于修正之前所有树组合产生的残差（误差）**，通过这种不断“查漏补缺”的方式逐步提升模型性能。

<img src="./assets/GDBT.png" alt="GDBT" style="zoom: 33%;" />

线性回归是调整权重，GBDT是调整模型。

### 训练流程

1. **初始化**：计算目标均值作为初始预测
2. **迭代建树**：计算当前预测与真实值的残差用新决策树拟合残差以学习率将新树预测累加到模型中
3. **重复**直到达到指定树的数量

### 预测流程

1. **从初始预测值开始**
2. **遍历每棵树**：样本按树的分裂规则走到叶节点获取该树的预测值
3. **加权求和**：初始值 + 学习率 × ∑各树预测值
4. **输出最终结果**

**核心**：串行训练，每树修正残差；并行预测，多树结果加权融合。