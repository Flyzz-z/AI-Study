# 机器学习

## 线性回归

### 基础概念

线性回归是通过找到最佳拟合直线来建立输入特征与输出值之间线性关系的算法。其数学表达式为：

$$
y = w_1x_1 + w_2x_2 + \cdots + w_nx_n + b = \mathbf{w}^T\mathbf{x} + b
$$

其中 $\mathbf{w}$ 是权重向量，$b$ 是偏置项，$\mathbf{x}$ 是输入特征向量。

### 损失函数

采用均方误差（MSE）作为损失函数：

$$
L(\mathbf{w}, b) = \frac{1}{m}\sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)})^2 = \frac{1}{m}\sum_{i=1}^m (\mathbf{w}^T\mathbf{x}^{(i)} + b - y^{(i)})^2
$$

其中 $m$ 是样本数量，$\hat{y}^{(i)}$ 是第 $i$ 个样本的预测值，$y^{(i)}$ 是真实值。

### 梯度下降算法

#### 核心思想
梯度下降是一种优化算法，通过迭代地调整参数来最小化损失函数。其基本思想是：沿着损失函数梯度的反方向更新参数，因为梯度指向函数增长最快的方向。

#### 参数更新规则

$$
\mathbf{w} := \mathbf{w} - \alpha \frac{\partial L}{\partial \mathbf{w}}
$$

$$
b := b - \alpha \frac{\partial L}{\partial b}
$$

其中 $\alpha$ 是学习率，控制每次更新的步长。

#### 梯度计算推导

对于单个样本 $(\mathbf{x}^{(i)}, y^{(i)})$：

1. **损失函数**：$L^{(i)} = (\hat{y}^{(i)} - y^{(i)})^2 = (\mathbf{w}^T\mathbf{x}^{(i)} + b - y^{(i)})^2$

2. **对权重的偏导数**：

$$
   \frac{\partial L^{(i)}}{\partial \mathbf{w}} = 2(\hat{y}^{(i)} - y^{(i)}) \cdot \frac{\partial \hat{y}^{(i)}}{\partial \mathbf{w}} = 2(\hat{y}^{(i)} - y^{(i)}) \cdot \mathbf{x}^{(i)}
   $$

3. **对偏置的偏导数**：

$$
   \frac{\partial L^{(i)}}{\partial b} = 2(\hat{y}^{(i)} - y^{(i)}) \cdot \frac{\partial \hat{y}^{(i)}}{\partial b} = 2(\hat{y}^{(i)} - y^{(i)}) \cdot 1
   $$

对于所有样本，取平均梯度：

$$
\frac{\partial L}{\partial \mathbf{w}} = \frac{2}{m}\sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)})\mathbf{x}^{(i)}
$$

$$
\frac{\partial L}{\partial b} = \frac{2}{m}\sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)})
$$

#### 算法流程

```python
# 初始化参数
w = np.random.randn(n_features) * 0.01
b = 0

# 梯度下降迭代
for iteration in range(max_iterations):
    # 前向传播：计算预测值
    y_pred = np.dot(X, w) + b
    
    # 计算误差
    error = y_pred - y
    
    # 计算梯度
    dw = (2/m) * np.dot(X.T, error)
    db = (2/m) * np.sum(error)
    
    # 参数更新
    w = w - learning_rate * dw
    b = b - learning_rate * db
```

### 带激活函数的线性模型

#### 模型结构

为了处理非线性问题，在线性变换后添加激活函数：

$$
z = \mathbf{w}^T\mathbf{x} + b \quad \text{(线性部分)}
$$

$$
\hat{y} = \sigma(z) \quad \text{(激活函数)}
$$

常见激活函数：

1. **Sigmoid函数**：$\sigma(z) = \frac{1}{1 + e^{-z}}$
   - 输出范围：(0, 1)
   - 导数：$\sigma'(z) = \sigma(z)(1 - \sigma(z))$

2. **ReLU函数**：$\sigma(z) = \max(0, z)$
   - 输出范围：[0, +∞)
   - 导数：$\sigma'(z) = \begin{cases} 1 & \text{if } z > 0 \\ 0 & \text{if } z \leq 0 \end{cases}$

#### 链式法则应用

对于带激活函数的模型，损失函数为：

$$
L = \frac{1}{m}\sum_{i=1}^m (\sigma(\mathbf{w}^T\mathbf{x}^{(i)} + b) - y^{(i)})^2
$$

使用链式法则计算梯度：

1. **链式分解**：

$$
   \frac{\partial L}{\partial \mathbf{w}} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot \frac{\partial z}{\partial \mathbf{w}}
   $$

2. **各部分的导数**：
   - $\frac{\partial L}{\partial \hat{y}} = \frac{2}{m}(\hat{y} - y)$
   - $\frac{\partial \hat{y}}{\partial z} = \sigma'(z)$ （激活函数导数）
   - $\frac{\partial z}{\partial \mathbf{w}} = \mathbf{x}$

3. **最终梯度**：

$$
   \frac{\partial L}{\partial \mathbf{w}} = \frac{2}{m}\sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)}) \cdot \sigma'(z^{(i)}) \cdot \mathbf{x}^{(i)}
   $$

$$
   \frac{\partial L}{\partial b} = \frac{2}{m}\sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)}) \cdot \sigma'(z^{(i)})
   $$

#### 向量化实现

```python
# 前向传播
z = np.dot(X, w) + b
y_pred = apply_activation(z)

# 反向传播
error = y_pred - y
activation_derivative = apply_activation_derivative(z)

# 计算梯度（链式法则）
dw = (2/m) * np.dot(X.T, error * activation_derivative)
db = (2/m) * np.sum(error * activation_derivative)

# 参数更新
w = w - learning_rate * dw
b = b - learning_rate * db
```

### 优化技巧

#### 1. 梯度裁剪

防止梯度爆炸：
```python
dw = np.clip(dw, -clip_threshold, clip_threshold)
db = np.clip(db, -clip_threshold, clip_threshold)
```

#### 2. 早停机制

监控验证损失，当连续多个epoch没有改善时停止训练：
```python
if abs(prev_loss - current_loss) < min_change:
    patience_counter += 1
    if patience_counter >= patience:
        break
else:
    patience_counter = 0
```

#### 3. 数值稳定性

使用float64精度，检查NaN值：
```python
if not (np.isnan(dw).any() or np.isnan(db)):
    w -= learning_rate * dw
    b -= learning_rate * db
```

### 算法特点

- **优点**：简单直观，计算效率高，易于实现
- **缺点**：可能陷入局部最优，对学习率敏感，特征缩放敏感
- **适用场景**：特征与目标存在线性或可分线性关系的问题


## 决策树

### 核心思想
决策树通过递归地将数据集分割成更纯的子集来构建预测模型，类似"20个问题"游戏。

### 关键参数
```python
DecisionTree(max_depth=3, min_samples_split=2, min_samples_leaf=1, task='classification')
```

- **max_depth**：控制树深度，防止过拟合
- **min_samples_split**：节点分裂最少样本数
- **min_samples_leaf**：叶节点最少样本数
- **task**：'classification'用基尼系数，'regression'用MSE

### 分裂准则
- **分类**：基尼系数 $Gini = 1 - Σ(p_k²)$，越小越纯
- **回归**：均方误差 $MSE = \frac{1}{N}Σ(y_i - ȳ)²$

### 预测过程
从根节点开始，根据特征值一路往下走，直到叶节点返回预测值。

### 优缺点
- **优点**：可解释性强，无需特征缩放，处理混合数据类型
- **缺点**：容易过拟合，对数据变化敏感

## 决策树与CART

信息熵：对于一个包含K个类别的数据集D，其信息熵 Ent(D) 为：$Ent(D) = -Σ (pk * log₂(pk))$（k从1到K）。

信息增益：$Gain(D, a) = Ent(D) - Σ (|Dᵥ| / |D|) * Ent(Dᵥ)$,`v是特征a的所有可能取值。 即**划分前的熵 - 划分后各子集的熵的加权平均**

### ID3

多叉树，选择特征将数据集划分为多少子集就有几叉。

<img src="./assets/ID3.png" alt="ID3" style="zoom: 25%;" />

### CART

同时解决分类和回归问题，二叉树。

<img src="./assets/CART.png" alt="CART" style="zoom:25%;" />

- **分类任务：基尼指数**

  基尼指数衡量的是从数据集中随机抽取两个样本，其类别标签不一致的概率。基尼指数越小，数据集的纯度越高。

  其计算公式为：$Gini(p) = 1 - Σ(p_k²)$，其中 `p_k`是样本属于第k类的概率。

  在分裂时，算法会计算每个可能的分裂点所产生的子集的**加权基尼指数**，并选择使该值最小的特征和分裂点。

- **回归任务：平方误差**

  对于回归问题，CART算法采用最小二乘准则，即最小化每个节点内样本的**平方误差和**。

  节点分裂的目标是找到那个特征和分裂点，使得分裂后左右两个子节点的平方误差之和最小。平方误差的计算公式通常为：$MSE = (1/N) * Σ(y_i - ȳ)^2$，其中 `ȳ`是该节点中所有样本目标值的均值。

**关于预剪枝**

预剪枝是一种“未雨绸缪”的防治策略，它在决策树的**构建过程中**就进行干预。通过预先设定一些规则（如树的最大深度、节点最小样本数等），在决定一个节点是否分裂前进行评估。如果分裂带来的性能提升不显著或不符合规则，则立即停止分裂，将其标记为叶节点。这种方法的优势在于**效率高、实现简单**，能有效防止过拟合。

**关于后剪枝**

后剪枝则是一种“先发展后治理”的优化策略，它**先放任决策树完全生长**（甚至允许其过拟合），然后再**自底向上**地对已生成的树进行修剪。它会评估各个子树，若将其替换为叶节点能提升模型在验证集上的泛化性能，则进行剪枝。这种方法的优势在于它具备了“全局视野”，保留了大量潜在分支的可能，因此得到的模型通常**泛化能力更强、性能更优**。

<img src="./assets/剪枝.png" alt="剪枝" style="zoom: 33%;" />



## 随机森林与GBDT

### Bagging

随机森林选择的是一种策略。它**不依赖于一棵“完美”的决策树**，而是**训练一大批（比如100棵或500棵）各具特色的决策树**，组成一个“森林”。当需要预测新数据时，让森林里的每棵树都进行投票（分类问题）或取平均值（回归问题），最终的预测结果由“多数票”或“平均意见”决定。这种方式能有效降低单一模型可能产生的过拟合风险，提升整体模型的稳定性和泛化能力。

<img src="./assets/随机森林.png" alt="随机森林" style="zoom: 33%;" />

两种随机性保证树之间的差异性：

1. 数据选择随机：对于每棵树的训练，它不是使用全部的数据集，而是**有放回地随机抽取一部分样本**。这种方法称为**Bootstrap抽样**。

2. **特征的随机选择**：在每棵树的每个节点进行分裂时，它不是考虑所有特征来寻找最佳分裂点，而是**随机的从全部特征中选取一部分特征候选**，每个树学习到的模式不同。

### Boosting

GBDT（Gradient Boosting Decision Tree，梯度提升决策树）是一种非常强大且应用广泛的机器学习算法，它通过**组合多个简单的决策树（弱学习器）**，来构建一个强大的预测模型。其核心思想是**每一棵新树都致力于修正之前所有树组合产生的残差（误差）**，通过这种不断“查漏补缺”的方式逐步提升模型性能。

<img src="./assets/GDBT.png" alt="GDBT" style="zoom: 33%;" />

线性回归是调整权重，GBDT是调整模型。

### 训练流程

1. **初始化**：计算目标均值作为初始预测
2. **迭代建树**：计算当前预测与真实值的残差用新决策树拟合残差以学习率将新树预测累加到模型中
3. **重复**直到达到指定树的数量

### 预测流程

1. **从初始预测值开始**
2. **遍历每棵树**：样本按树的分裂规则走到叶节点获取该树的预测值
3. **加权求和**：初始值 + 学习率 × ∑各树预测值
4. **输出最终结果**

**核心**：串行训练，每树修正残差；并行预测，多树结果加权融合。